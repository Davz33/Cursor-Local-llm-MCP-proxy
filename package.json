{
  "name": "local-llm-proxy-mcp",
  "version": "1.2.0",
  "description": "Enhanced MCP server for local LLM with RAG, agentic capabilities, and tool integration using LlamaIndex.TS",
  "type": "module",
  "main": "dist/index.js",
  "scripts": {
    "build": "tsc",
    "start": "node dist/index.js",
    "start:with-validation": "LLM_VALIDATION_ENABLED=true USE_LOCAL_VALIDATOR=true node dist/index.js",
    "dev": "tsx index.ts",
    "dev:with-validation": "LLM_VALIDATION_ENABLED=true USE_LOCAL_VALIDATOR=true tsx index.ts",
    "dev:watch": "nodemon --exec \"tsx index.ts\"",
    "clean": "rm -rf dist",
    "prebuild": "npm run clean"
  },
  "dependencies": {
    "@llamaindex/huggingface": "^0.1.28",
    "@llamaindex/openai": "^0.4.18",
    "@modelcontextprotocol/sdk": "^0.4.0",
    "dotenv": "^17.2.2",
    "llamaindex": "^0.11.28",
    "node-fetch": "^3.3.2",
    "zod": "^3.22.0"
  },
  "keywords": [
    "mcp",
    "llm",
    "proxy",
    "rag",
    "agentic",
    "llamaindex",
    "lm-studio",
    "local-llm"
  ],
  "author": "Davide Vitiello",
  "license": "MIT",
  "devDependencies": {
    "@types/node": "^24.3.0",
    "nodemon": "^3.1.10",
    "ts-node": "^10.9.2",
    "tsx": "^4.20.5",
    "typescript": "^5.9.2"
  }
}
