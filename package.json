{
  "name": "local-llm-proxy-mcp",
  "version": "1.0.0",
  "description": "MCP server for local LLM with speculative decoding and fallback",
  "type": "module",
  "main": "index.js",
  "scripts": {
    "start": "node index.js",
    "start:with-validation": "LLM_VALIDATION_ENABLED=true USE_LOCAL_VALIDATOR=true node index.js"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "^0.4.0",
    "node-fetch": "^3.3.2",
    "llamaindex": "^0.1.0",
    "@llamaindex/openai": "^0.1.0",
    "@llamaindex/huggingface": "^0.1.0",
    "zod": "^3.22.0"
  },
  "keywords": [
    "mcp",
    "llm",
    "proxy",
    "speculative-decoding"
  ],
  "author": "Davide Vitiello",
  "license": "MIT"
}

